from aiofiles.os import path as aiopath, listdir, remove
from asyncio import sleep, gather
from html import escape
from requests import utils as rutils
from ...helper.mirror_leech_utils.status_utils.qbit_status import QbittorrentStatus

from ... import (
    intervals,
    task_dict,
    task_dict_lock,
    LOGGER,
    non_queued_up,
    non_queued_dl,
    queued_up,
    queued_dl,
    queue_dict_lock,
    same_directory_lock,
    DOWNLOAD_DIR,
)
from ...core.config_manager import Config
from ...core.torrent_manager import TorrentManager
from ..common import TaskConfig
from ..ext_utils.bot_utils import sync_to_async
from ..ext_utils.db_handler import database
from ..ext_utils.files_utils import (
    get_path_size,
    clean_download,
    clean_target,
    join_files,
    create_recursive_symlink,
    remove_excluded_files,
    move_and_merge,
)
from ..ext_utils.links_utils import is_gdrive_id
from ..ext_utils.status_utils import get_readable_file_size
from ..ext_utils.task_manager import start_from_queued, check_running_tasks
from ..mirror_leech_utils.gdrive_utils.upload import GoogleDriveUpload
from ..mirror_leech_utils.rclone_utils.transfer import RcloneTransferHelper
from ..mirror_leech_utils.status_utils.gdrive_status import GoogleDriveStatus
from ..mirror_leech_utils.status_utils.queue_status import QueueStatus
from ..mirror_leech_utils.status_utils.rclone_status import RcloneStatus
from ..mirror_leech_utils.status_utils.telegram_status import TelegramStatus
from ..mirror_leech_utils.telegram_uploader import TelegramUploader
from ..telegram_helper.button_build import ButtonMaker
from ..telegram_helper.message_utils import (
    send_message,
    delete_status,
    update_status_message,
)


class TaskListener(TaskConfig):
    def __init__(self):
        super().__init__()

    async def clean(self):
        try:
            if st := intervals["status"]:
                for intvl in list(st.values()):
                    intvl.cancel()
            intervals["status"].clear()
            await gather(TorrentManager.aria2.purgeDownloadResult(), delete_status())
        except:
            pass

    def clear(self):
        self.subname = ""
        self.subsize = 0
        self.files_to_proceed = []
        self.proceed_count = 0
        self.progress = True

    async def remove_from_same_dir(self):
        async with task_dict_lock:
            if (
                    self.folder_name
                    and self.same_dir
                    and self.mid in self.same_dir[self.folder_name]["tasks"]
            ):
                self.same_dir[self.folder_name]["tasks"].remove(self.mid)
                self.same_dir[self.folder_name]["total"] -= 1

    async def start_next_batch(self):
        if self.is_cancelled:
            return

        if not self.all_files:
            LOGGER.info(f"All batches have been processed for: {self.name}")
            # –≠—Ç–æ—Ç –≤—ã–∑–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–º, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            if not self.is_leech:  # –î–ª—è mirror/clone —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏–∑ on_upload_complete
                await self.on_upload_complete(None, None, None, None)
            return

        self.current_batch_files.clear()
        current_batch_size = 0
        file_ids_for_this_batch = []

        # 1. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø–∞–∫–µ—Ç
        temp_files_for_next_round = []
        for file_info in self.all_files:
            file_size = file_info.size
            # –ï—Å–ª–∏ –ø–∞–∫–µ—Ç –ø—É—Å—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–µ batch_size
            if not self.current_batch_files:
                self.current_batch_files.append(file_info)
                file_ids_for_this_batch.append(file_info.index)
                current_batch_size += file_size
            elif current_batch_size + file_size <= self.batch_size:
                self.current_batch_files.append(file_info)
                file_ids_for_this_batch.append(file_info.index)
                current_batch_size += file_size
            else:
                temp_files_for_next_round.append(file_info)

        self.all_files = temp_files_for_next_round

        if not self.current_batch_files:
            LOGGER.error(f"Error creating new batch for {self.name}. No files selected.")
            await self.on_download_error("Failed to create a new batch.")
            return

        LOGGER.info(
            f"Starting batch #{len(self.current_batch_files)} files for {self.name}. Size: {get_readable_file_size(current_batch_size)}")

        try:
            # 2. –£–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª–∞–º–∏ –≤ qBittorrent
            task = await get_task_by_gid(self.gid)
            ext_hash = task.hash()

            # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–∫–ª—é—á–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ —Ç–æ—Ä—Ä–µ–Ω—Ç–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0)
            all_file_ids = [f.index for f in (await TorrentManager.qbittorrent.torrents.files(ext_hash))]
            await TorrentManager.qbittorrent.torrents.file_prio(hash=ext_hash, id=all_file_ids, priority=0)

            # –ó–∞—Ç–µ–º –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞
            await TorrentManager.qbittorrent.torrents.file_prio(hash=ext_hash, id=file_ids_for_this_batch, priority=1)

            # 3. –ó–∞–ø—É—Å–∫–∞–µ–º/–≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
            await TorrentManager.qbittorrent.torrents.start([ext_hash])

            # 4. –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—É—Å-–æ–±—ä–µ–∫—Ç, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
            async with task_dict_lock:
                if self.mid not in task_dict:
                    task_dict[self.mid] = QbittorrentStatus(self)
                    await self.on_download_start()
                    if self.multi <= 1:
                        await send_status_message(self.message)
        except Exception as e:
            LOGGER.error(f"Error while starting next batch for {self.name}: {e}")
            await self.on_download_error(str(e))

    async def on_download_start(self):
        if (
                self.is_super_chat
                and Config.INCOMPLETE_TASK_NOTIFIER
                and Config.DATABASE_URL
        ):
            await database.add_incomplete_task(
                self.message.chat.id, self.message.link, self.tag
            )

    async def on_download_complete(self):
        await sleep(2)
        if self.is_cancelled:
            return
        multi_links = False
        if (
                self.folder_name
                and self.same_dir
                and self.mid in self.same_dir[self.folder_name]["tasks"]
        ):
            async with same_directory_lock:
                while True:
                    async with task_dict_lock:
                        if self.mid not in self.same_dir[self.folder_name]["tasks"]:
                            return
                        if (
                                self.same_dir[self.folder_name]["total"] <= 1
                                or len(self.same_dir[self.folder_name]["tasks"]) > 1
                        ):
                            if self.same_dir[self.folder_name]["total"] > 1:
                                self.same_dir[self.folder_name]["tasks"].remove(
                                    self.mid
                                )
                                self.same_dir[self.folder_name]["total"] -= 1
                                spath = f"{self.dir}{self.folder_name}"
                                des_id = list(self.same_dir[self.folder_name]["tasks"])[
                                    0
                                ]
                                des_path = f"{DOWNLOAD_DIR}{des_id}{self.folder_name}"
                                LOGGER.info(f"Moving files from {self.mid} to {des_id}")
                                await move_and_merge(spath, des_path, self.mid)
                                multi_links = True
                            break
                    await sleep(1)
        async with task_dict_lock:
            if self.is_cancelled:
                return
            if self.mid not in task_dict:
                return
            download = task_dict[self.mid]
            self.name = download.name()
            gid = download.gid()
        LOGGER.info(f"Download completed: {self.name}")

        if not (self.is_torrent or self.is_qbit):
            self.seed = False

        if multi_links:
            self.seed = False
            await self.on_upload_error(
                f"{self.name} Downloaded!\n\nWaiting for other tasks to finish..."
            )
            return
        elif self.same_dir:
            self.seed = False

        if self.is_batch:
            LOGGER.info(f"Batch download completed for: {self.name}")
            await self.on_upload_start(self.dir)
            return

        if self.folder_name:
            self.name = self.folder_name.strip("/").split("/", 1)[0]

        if not await aiopath.exists(f"{self.dir}/{self.name}"):
            try:
                files = await listdir(self.dir)
                self.name = files[-1]
                if self.name == "yt-dlp-thumb":
                    self.name = files[0]
            except Exception as e:
                await self.on_upload_error(str(e))
                return

        dl_path = f"{self.dir}/{self.name}"
        self.size = await get_path_size(dl_path)
        self.is_file = await aiopath.isfile(dl_path)

        if self.seed:
            up_dir = self.up_dir = f"{self.dir}10000"
            up_path = f"{self.up_dir}/{self.name}"
            await create_recursive_symlink(self.dir, self.up_dir)
            LOGGER.info(f"Shortcut created: {dl_path} -> {up_path}")
        else:
            up_dir = self.dir
            up_path = dl_path

        await remove_excluded_files(self.up_dir or self.dir, self.excluded_extensions)

        if not Config.QUEUE_ALL:
            async with queue_dict_lock:
                if self.mid in non_queued_dl:
                    non_queued_dl.remove(self.mid)
            await start_from_queued()

        if self.join and not self.is_file:
            await join_files(up_path)

        if self.extract and not self.is_nzb:
            up_path = await self.proceed_extract(up_path, gid)
            if self.is_cancelled:
                return
            if self.is_batch:
                # –û—á–∏—â–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞
                await clean_download(self.dir)

                # –ï—Å–ª–∏ –µ—â–µ –µ—Å—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø–∞–∫–µ—Ç
                if self.all_files:
                    await send_message(self.message,
                                       f"Batch uploaded. Starting next batch for <code>{self.name}</code>.")
                    await self.start_next_batch()
                    return  # –í–ê–ñ–ù–û: –í—ã—Ö–æ–¥–∏–º, —á—Ç–æ–±—ã –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                else:
                    # –≠—Ç–æ –±—ã–ª –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞–∫–µ—Ç, –ø–æ–∑–≤–æ–ª—è–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å—Å—è –æ—Å—Ç–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏
                    LOGGER.info(f"All batches uploaded for: {self.name}")
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
            self.size = await get_path_size(up_dir)
            self.clear()
            await remove_excluded_files(up_dir, self.excluded_extensions)

        if self.ffmpeg_cmds:
            up_path = await self.proceed_ffmpeg(
                up_path,
                gid,
            )
            if self.is_cancelled:
                return
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
            self.size = await get_path_size(up_dir)
            self.clear()

        if self.name_sub:
            up_path = await self.substitute(up_path)
            if self.is_cancelled:
                return
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]

        if self.screen_shots:
            up_path = await self.generate_screenshots(up_path)
            if self.is_cancelled:
                return
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
            self.size = await get_path_size(up_dir)

        if self.convert_audio or self.convert_video:
            up_path = await self.convert_media(
                up_path,
                gid,
            )
            if self.is_cancelled:
                return
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
            self.size = await get_path_size(up_dir)
            self.clear()

        if self.sample_video:
            up_path = await self.generate_sample_video(up_path, gid)
            if self.is_cancelled:
                return
            self.is_file = await aiopath.isfile(up_path)
            self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
            self.size = await get_path_size(up_dir)
            self.clear()

        if self.compress:
            up_path = await self.proceed_compress(
                up_path,
                gid,
            )
            self.is_file = await aiopath.isfile(up_path)
            if self.is_cancelled:
                return
            self.clear()

        self.name = up_path.replace(f"{up_dir}/", "").split("/", 1)[0]
        self.size = await get_path_size(up_dir)

        if self.is_leech and not self.compress:
            await self.proceed_split(up_path, gid)
            if self.is_cancelled:
                return
            self.clear()

        self.subproc = None

        add_to_queue, event = await check_running_tasks(self, "up")
        await start_from_queued()
        if add_to_queue:
            LOGGER.info(f"Added to Queue/Upload: {self.name}")
            async with task_dict_lock:
                task_dict[self.mid] = QueueStatus(self, gid, "Up")
            await event.wait()
            if self.is_cancelled:
                return
            LOGGER.info(f"Start from Queued/Upload: {self.name}")

        self.size = await get_path_size(up_dir)

        if self.is_leech:
            LOGGER.info(f"Leech Name: {self.name}")
            tg = TelegramUploader(self, up_dir)
            async with task_dict_lock:
                task_dict[self.mid] = TelegramStatus(self, tg, gid, "up")
            await gather(
                update_status_message(self.message.chat.id),
                tg.upload(),
            )
            del tg
        elif is_gdrive_id(self.up_dest):
            LOGGER.info(f"Gdrive Upload Name: {self.name}")
            drive = GoogleDriveUpload(self, up_path)
            async with task_dict_lock:
                task_dict[self.mid] = GoogleDriveStatus(self, drive, gid, "up")
            await gather(
                update_status_message(self.message.chat.id),
                sync_to_async(drive.upload),
            )
            del drive
        else:
            LOGGER.info(f"Rclone Upload Name: {self.name}")
            RCTransfer = RcloneTransferHelper(self)
            async with task_dict_lock:
                task_dict[self.mid] = RcloneStatus(self, RCTransfer, gid, "up")
            await gather(
                update_status_message(self.message.chat.id),
                RCTransfer.upload(up_path),
            )
            del RCTransfer
        return

    async def on_upload_complete(
            self, link, files, folders, mime_type, rclone_path="", dir_id=""
    ):
        if (
                self.is_super_chat
                and Config.INCOMPLETE_TASK_NOTIFIER
                and Config.DATABASE_URL
        ):
            await database.rm_complete_task(self.message.link)
        msg = f"<b>Name: </b><code>{escape(self.name)}</code>\n\n<b>Size: </b>{get_readable_file_size(self.size)}"
        LOGGER.info(f"Task Done: {self.name}")
        if self.is_leech:
            msg += f"\n<b>Total Files: </b>{folders}"
            if mime_type != 0:
                msg += f"\n<b>Corrupted Files: </b>{mime_type}"
            msg += f"\n<b>cc: </b>{self.tag}\n\n"
            if not files:
                await send_message(self.message, msg)
            else:
                fmsg = ""
                for index, (link, name) in enumerate(files.items(), start=1):
                    fmsg += f"{index}. <a href='{link}'>{name}</a>\n"
                    if len(fmsg.encode() + msg.encode()) > 4000:
                        await send_message(self.message, msg + fmsg)
                        await sleep(1)
                        fmsg = ""
                if fmsg != "":
                    await send_message(self.message, msg + fmsg)
        else:
            msg += f"\n\n<b>Type: </b>{mime_type}"
            if mime_type == "Folder":
                msg += f"\n<b>SubFolders: </b>{folders}"
                msg += f"\n<b>Files: </b>{files}"
            if (
                    link
                    or rclone_path
                    and Config.RCLONE_SERVE_URL
                    and not self.private_link
            ):
                buttons = ButtonMaker()
                if link:
                    buttons.url_button("‚òÅÔ∏è Cloud Link", link)
                else:
                    msg += f"\n\nPath: <code>{rclone_path}</code>"
                if rclone_path and Config.RCLONE_SERVE_URL and not self.private_link:
                    remote, rpath = rclone_path.split(":", 1)
                    url_path = rutils.quote(f"{rpath}")
                    share_url = f"{Config.RCLONE_SERVE_URL}/{remote}/{url_path}"
                    if mime_type == "Folder":
                        share_url += "/"
                    buttons.url_button("üîó Rclone Link", share_url)
                if not rclone_path and dir_id:
                    INDEX_URL = ""
                    if self.private_link:
                        INDEX_URL = self.user_dict.get("INDEX_URL", "") or ""
                    elif Config.INDEX_URL:
                        INDEX_URL = Config.INDEX_URL
                    if INDEX_URL:
                        share_url = f"{INDEX_URL}/findpath?id={dir_id}"
                        buttons.url_button("‚ö° Index Link", share_url)
                        if mime_type.startswith(("image", "video", "audio")):
                            share_urls = f"{INDEX_URL}/findpath?id={dir_id}&view=true"
                            buttons.url_button("üåê View Link", share_urls)
                button = buttons.build_menu(2)
            else:
                msg += f"\n\nPath: <code>{rclone_path}</code>"
                button = None
            msg += f"\n\n<b>cc: </b>{self.tag}"
            await send_message(self.message, msg, button)
        if self.seed:
            await clean_target(self.up_dir)
            async with queue_dict_lock:
                if self.mid in non_queued_up:
                    non_queued_up.remove(self.mid)
            await start_from_queued()
            return
        await clean_download(self.dir)
        async with task_dict_lock:
            if self.mid in task_dict:
                del task_dict[self.mid]
            count = len(task_dict)
        if count == 0:
            await self.clean()
        else:
            await update_status_message(self.message.chat.id)

        async with queue_dict_lock:
            if self.mid in non_queued_up:
                non_queued_up.remove(self.mid)

        await start_from_queued()

    async def on_download_error(self, error, button=None):
        async with task_dict_lock:
            if self.mid in task_dict:
                del task_dict[self.mid]
            count = len(task_dict)
        await self.remove_from_same_dir()
        msg = f"{self.tag} Download: {escape(str(error))}"
        await send_message(self.message, msg, button)
        if count == 0:
            await self.clean()
        else:
            await update_status_message(self.message.chat.id)

        if (
                self.is_super_chat
                and Config.INCOMPLETE_TASK_NOTIFIER
                and Config.DATABASE_URL
        ):
            await database.rm_complete_task(self.message.link)

        async with queue_dict_lock:
            if self.mid in queued_dl:
                queued_dl[self.mid].set()
                del queued_dl[self.mid]
            if self.mid in queued_up:
                queued_up[self.mid].set()
                del queued_up[self.mid]
            if self.mid in non_queued_dl:
                non_queued_dl.remove(self.mid)
            if self.mid in non_queued_up:
                non_queued_up.remove(self.mid)

        await start_from_queued()
        await sleep(3)
        await clean_download(self.dir)
        if self.up_dir:
            await clean_download(self.up_dir)
        if self.thumb and await aiopath.exists(self.thumb):
            await remove(self.thumb)

    async def on_upload_error(self, error):
        async with task_dict_lock:
            if self.mid in task_dict:
                del task_dict[self.mid]
            count = len(task_dict)
        await send_message(self.message, f"{self.tag} {escape(str(error))}")
        if count == 0:
            await self.clean()
        else:
            await update_status_message(self.message.chat.id)

        if (
                self.is_super_chat
                and Config.INCOMPLETE_TASK_NOTIFIER
                and Config.DATABASE_URL
        ):
            await database.rm_complete_task(self.message.link)

        async with queue_dict_lock:
            if self.mid in queued_dl:
                queued_dl[self.mid].set()
                del queued_dl[self.mid]
            if self.mid in queued_up:
                queued_up[self.mid].set()
                del queued_up[self.mid]
            if self.mid in non_queued_dl:
                non_queued_dl.remove(self.mid)
            if self.mid in non_queued_up:
                non_queued_up.remove(self.mid)

        await start_from_queued()
        await sleep(3)
        await clean_download(self.dir)
        if self.up_dir:
            await clean_download(self.up_dir)
        if self.thumb and await aiopath.exists(self.thumb):
            await remove(self.thumb)
